\section{Contributions}
\paragraph{Algorithmic}
\begin{itemize}
    \item adaptive noise handling
    \item parallelization
\end{itemize}
\paragraph{Software architecture}
\begin{itemize}
    \item modularity: making each of the components (fitters, filters, etc) easily changeable
    \item ability to have scalar and least-squares optimizers in one code base.
\end{itemize}
\paragraph{Open source}
\begin{itemize}
    \item we make the algorithm available in estimagic.
    \item fast python implementations of sub-solvers (bntr and gqtpar)
    \item python implementations of the  more-wild and cartis-roberts benchmark problems
\end{itemize}

\section{Motivation}

\begin{itemize}
    \item the four problems from slides
    \item properties of residuals: continuously differnetiable but Jacobian not available
    \item examples
        \begin{itemize}
            \item check for examples of pure least-squares problem application
            \item MSM and GMM and cholesky trick
            \item calibration of engineering problems (Janos)
            \item the covid model
        \end{itemize}
\end{itemize}

\section{Literature review}
\begin{itemize}
    \item BOBYQA
        \begin{itemize}
            \item parts of BOBYQA that explain how other optimizers came around
            \item quadratic models
            \item under-determined fitting (with 2n+1 points) with penalty on hessian change
        \end{itemize}
        \item POUNDERS
            \begin{itemize}
                \item literal translation of bobyqa to least-squares.
                \item same number of points and same quadratic model used for fitting each residual and aggregation to the scalar model.
            \end{itemize}
        \item DFOLS
            \begin{itemize}
                \item linear residual models aggregated into quadratic scalar model
                \item noise handling via user-specified sequences
            \end{itemize}
    \item ASTRO-DF (JANOS)
            \begin{itemize}
                \item scalar optimizer
                \item adaptive noise handling
            \end{itemize}
    \item Parallel Neldermead
            \begin{itemize}
                \item using parallelism even though the objective function is not parallelized
            \end{itemize}
    \item refer to wild paper
\end{itemize}





\section{Tranquilo core algorithm}


\subsection{The main loop}
%%TODO
% 1. define notation for the building blocks write equations for the interface and data flow
% 2. write down algorithm listing
In each iteration $t$, given current candidate point $x^*_t$ and search radius $\Delta^{search}_t$, we begin by screening the hisotry of criterion evaluations to retrieve all points that lie within the neighbourhood $\Delta^{search}_t=\gamma^{search}\Delta_t^{tr}$ of $x_t$:
\begin{align}
    \mathcal{E}_t\equiv\{x\in\mathcal{H}|\Hquad\lVert x^*_t-x\rVert\leq\Delta^{search}_t\}
    \label{eq:hist-search}
\end{align}
We filter points to potentially improve the quality of the sample points:
\begin{align}
    \mathcal{E}'_t\equiv Filter (\mathcal{E}_t) = \{ Filter (x)|\Hquad x\in \mathcal{E}_t\}
    \label{eq:filtering}
\end{align}
If the size of the filtered sample is smaller than the target sample size ($p+1$, usually), we sample new points in the current trust region:

\begin{align}
    \mathcal{N}\equiv\mathbb{B}(\mathcal{E}_t^{'},R_t,n^{target})
\end{align}
where $\mathbb{B}$ is a sampling function\comment[]{MP:should talk about bounds and optimal sampling, etc here?}

We approximate the vector function $\mathbf{f}$ by fitting a vector model to sample points $x_t\in\mathcal{S}_t\equiv\mathcal{E}^{'}_t\cup\mathcal{N}_t$ and either the corresponding vector-function evaluations $y_t=\mathbf{f}(x_t)$, or, to introduce momentum, the residuals of the model from the previous iteration, $\tilde{y}_t =y_t-M_{t-1}^v(x_t)$:
\begin{align}
    \mathbf{f}\approx M_t^v\equiv Fitter(x_t,y_t;R_t,M_{t-1}^v)
\end{align}
where the fitter function is constructed such that it's applicable for both regression ($|\mathcal{N}_t|\geq p$) and interpolation ($|\mathcal{N}_t|\leq p$) problems. Additional dependency on the trust region $R_t$ is necessary to scale the model to a unit circle to facilitate solving for the scalar model's,$M_t^s = \lVert M_t^v\rVert$, optimum.

To find a candidate step $s_t$, we solve for the optimum of the aggregate model:\comment[]{equation 2.10 in DFOLS paper imposes condition on the step side to }
\begin{align}
    s_t\approx arg\,\min\limits_sM_t^v(x_t+s)
    \label{eq:cand-step}
\end{align}


As a safety measure, to avoid stagnation, we impose a lower threshold on the candidate step size $\lVert s_t\rVert$. To this end, we iteratively improve sample quality, fit the model and solve for the step size until we obtain a large enough candidate step.

We evaluate the full criterion function $F$ at the new candidate point $x_t+s_t$. We accept the candidate step and expand the trust region radius $\Delta_t^{tr}$ if we observe a large enough decrease in the function value, relative to the decrease predicted by model $M_t^s$:
\begin{align}
    \rho_t\equiv\frac{F(x_t)-F(x_t+s_t)}{M^s(x_t)-M^s(x_t+s_t)}\geq\rho_{min}
    \label{eq:rho}
\end{align}
If we don't observe a sufficient improvement in the function value, we reject the step (setting $x^{*}_{t+1}=x_t^{*}$) and shrink the trust region radius. The algorithm terminates either when a given convergence criterion (successful termination) or the maximum number of iterations is achieved.

We summarize the Tranquilo core algorithm in Algorithm \ref{algo:core}:

\begin{algorithm}[H]
    \caption{Tranquilo main loop}\label{algo:core}
    \KwIn{
    Starting point $x^*_0\in\mathbb{R}^p$, initial trust region radius $\Delta^{tr}_0$, and target sample size $n^{target}$.

    Initial sample $\mathcal{S}_0\subset R(x_0,\Delta_0^{tr})$ with $|\mathcal{S}_0|=n^{target}$

    Other parameters: Search factor $\gamma^{search}$, minimum step size $s^{min}$, minimum relative improvement $\rho^{min}$, trust region expansion and shrinking factors $0<\gamma^{dec}<1<\gamma^{inc}$, and maximum number of iterations $t^{max}$


    }
    \For{t=0,1,2,...}{
        Given $x_t^*$ and $\Delta_t^{search}=\gamma^{search}\Delta^{tr}_{t}$, scan history for points in the search region: $\mathcal{E}_t = \{x\in\mathcal{H}|\Hquad\lVert x^*_t-x\rVert\leq\Delta^{search}_t\}$\\
        Filter existing points: $\mathcal{E}'_t\equiv Filter (\mathcal{E}_t) = \{ Filter (x)|\Hquad x\in \mathcal{E}_t\}$\\
        \If{$|E_t^{'}|<n^{target}$}{
            Sample new points in the trust region: $\mathcal{N}_t\equiv\mathbb{B}(\mathcal{E}_t^{'},R_t,n^{target})$
        }
        Given $x_t^*$ and $\mathcal{S}_t\equiv\mathcal{E}^{'}_t\cup\mathcal{N}_t$, build the vector model $M_t^v\equiv Fitter(x_t,y_t;R_t,M_{t-1}^v)$\label{algo-core-line:fit}\\
        Solve for the optimum of the scalar model $M_t^s$: $s_t\approx arg\,\min\limits_sM_t^s(x_t+s)$\label{algo-core-line:solve}\\
        \If{$|\mathcal{S}_t|>n^{target}$}{
            \While{$s_t\leq s^{min}$}{
                Drop sample points\\
                Build the vector model (step \ref{algo-core-line:fit})\\
            Solve for the optimum of scalar model (step \ref{algo-core-line:solve})
            }
        }

        \While{$\lVert s_t\rVert\leq s^{min}$}{
            Drop points and resample\\
            Build the vector model (step \ref{algo-core-line:fit})\\
            Solve for the optimum of scalar model (step \ref{algo-core-line:solve})
        }
        \nonl Acceptance decision:\\
         Calculate $\rho_t=\frac{F(x^{*}_t)-F(x_t^{*}+s_t)}{M^s_t(x^{*}_t)-M^s_t(x_t^{*}+s_t)}$\\
         \eIf{$\rho\geq\rho^{min}$}{
         Set $x_{t+1}^{*} = x_t^{*} + s_t$ and increase trust region radius $\Delta^{tr}_{t+1}=\gamma^{inc}\Delta^{tr}_t$

        }{
            Set $x_{t+1}^{*}=x_{t}^*$ and shrink trust region radius $\Delta^{tr}_{t+1}=\gamma^{dec}\Delta^{tr}_t$

        }
    \If{$t>t^{max}$ or $converged$}{break}


    }
    \end{algorithm}

\subsection{Implementation and benchmarking}

\paragraph{Implementation details} Describe the specifics of the following in the baseline:



We then fit linear models for each residual $f_i(x)$:\comment[]{MP: Need to discuss indexing}
\begin{align}
    f_i(x)\approx m_{ik}(x) \equiv a_{ik}+b_{ik}^Tx
    \label{eq:model-linear}
\end{align}
by solving the following least-squares problem for each residual:
\begin{align}
    \min\limits_{a_{ik},b_{ik}}\sum\limits_{t=1}^{|\mathbb{S}'_t|}\big(f_i(x_t)-(a_{ik}+b_{ik}^Tx_t)\big)^2
    \label{eq:fit-linear}
\end{align}




The approximation to the full objective $F$ can then be constructed as the inner product of the linear models \ref{eq:model-linear}:
\begin{align}
    F(x)\approx M_t(x)\equiv\sum\limits_im_{ik}(x)^2 &= \sum\limits_i a_{ik}^2 + \sum\limits_i 2a_{ik} b_{ik}^T x + \sum_i x^T b_{ik}b_{ik}^T x\nonumber\\
    &\equiv \alpha + g_t^T x + \frac{1}{2} x^T H_t x
    \label{eq:model-full}
\end{align}

\begin{itemize}
    \item filters: we keep all
    \item fitting: we use ols
    \item subsolvers: we use bntr for cube sampling (bounded problems) and gqtpar for sphere sampling
\end{itemize}
\paragraph{Benchmark plots}
\begin{itemize}
    \item describe the more-wild benchmark set ???
    \item describe the plot axes:
        \begin{itemize}
            \item x: number of function evaluations normalized by the best result
            \item y: share of the problems solved
        \end{itemize}
    \item show profile plot (only ls optimizers???)
    \item idea: show all optimizers to show scalar optimizers are not fit for the ls problems?
\end{itemize}


\section{Parallelization}
\subsection{Implementation}
\begin{itemize}
\item describe the implementation details
\begin{itemize}
    \item if batch size is 1 (no parallelization): do classic acceptance based on rhos
    \item else, check if the candidate point is at the tr border
    \item if yes, get number of line search points (at most 3, at least batch size-1) and define the grid on the line
    \item calculate number of unallocated function evals based on the number of points on the line search and batch size ($batch_size-1-n_evals_line_search$) WHY $batch_size-1$ ?????
    \item if the previous number is non-zero, do speculative sampling around the candidate point using search radius
    \item add the line search and speculative points, if any, to the history
    \item check if criterion is smaller at any of the new points (spec+line search)
    \item if so, update candidate fval and x
\end{itemize}
\item discuss optimizers that implement parallel evaluations: dfols since it is the only ls optimizer??
\end{itemize}

\subsection{Benchmarking}
\begin{itemize}
    \item discuss the cost model
    \item motivate the cost model
    \item show profile plot
    \item discuss profile plot
\end{itemize}

\section{Noisy optimization}

\subsection{Implementation}
pretty much follow the slides
\begin{itemize}
    \item estimation of the variance
    \item calculation of rho noise with formula -> number of function evals at each sample point
    \item power analysis step with formula -> number of function evaluations at candidate point and trsutregion center for acceptance decision
\end{itemize}
\subsection{Benchmarking}
\begin{itemize}
    \item discuss the specific noise-related parameters in tranquilo
    \item discuss specific noise-related parameters in df-ols (number of function evals)
\end{itemize}

\section{conclusion}
