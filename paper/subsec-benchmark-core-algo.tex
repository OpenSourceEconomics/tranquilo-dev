\subsection{Benchmarking}
\label{subsec:benchmark-core-algo}
\begin{itemize}
    \item describe the more-wild benchmark set
    \item show plot competition\_ls
    \item describe the plot axes:
    \begin{itemize}
            \item x: number of function evaluations normalized by the best result
            \item y: share of the problems solved
        \end{itemize}
        \item show plot scalar\_and\_ls to highlight that scalar optimizers are not well fit for ls problems
    \end{itemize}

% Benchmarking Derivative-Free Optimization Solvers

\begin{figure}
    \includegraphics[width=\textwidth]{../bld/figures/profile_plots/competition_ls}
    \caption{Benchmark plots for the least squares case.}
    \label{fig:core::benchmark-ls}
\end{figure}\comment[id=TM]{Benchmark plot: Need to remove default line and remove the "experimental" suffix.}

\begin{figure}
    \includegraphics[width=\textwidth]{../bld/figures/profile_plots/competition_scalar}
    \caption{Benchmark plots for the scalar case.}
    \label{fig:core::benchmark-scalar}
\end{figure}\comment[id=TM]{Benchmark plot: Need to remove default line and remove the "experimental" suffix.}

\noindent A benchmark is defined in terms of (i) a set of problems, (ii) a set of solvers, and
(iii) a convergence test \cite{Dolan2002}. We focus on the class of least squares problems, where there is a limit on the user's computational budget, the objective function is expensive to evaluate and derivates are not readily available.
Thus we follow the benchmarking procedure of \cite{MoreWild2009} for benchmarking derivative-free optimization algorithms. They provide (i) a set of 53 smooth non-linear least squares problems and (iii) a convergence test suitable for situations where the objective function is expensive to evaluate and derivates may not be attainable.
Moreover, they introduce profile plots as a visualization tool to compare the performance of solvers across the benchmark set.
Profile plots - referred to as \textit{performance profiles} in \cite{MoreWild2009} - show the percentage of problems solved by each algorithm within a given computational budget, scaled by the time the fastest algorithm took to solve the problem. The computational budget of a user is represented by a quantifiable runtime measure, such as the total number of function evaluations.
% Derivate-free optimization, expensive function evaluations, and derivatives are not readily available.
% Hence, MW propose a convergence test that measures the decrease in the objective function :
The MorÃ© and Wild (MW) benchmark set contains 53 test cases for non-linear least squares solvers.
These test cases are constructed based on 22 functions, originally derived from the CUTEr
Problems (\cite{Gould2003}). The MW benchmark set has been used to benchmark all modern model-based non-linear derivative-free least squares solvers, such as POUNDERS (\cite{Wild2015}), DFOGN (\cite{Cartis2017}), and DFOLS (\cite{Cartis2019}).
% Their benchmark set contains 53 smooth problems.
% and are based on 22 nonlinear least squares functions, which were defined in {gould2015cuter}.
% The problems are taken from the CUTEr collection {gould2015cuter} and
% the MINPACK-1 collection {more1980user}. The Mw benchmark set is available in the \texttt{R} package
% \texttt{minqa} {wild2016minqa}.
% comprises 22 nonlinear least squares functions defined in the
% CUTEr [9] collection.


\noindent The 53 test cases are defined by a twice continuously differentiable function of the form described in \ref{eq:problem-ls-det}. The parameter dimensions are quite small, varying between 2 and 12, where the median dimension is 7.
The dimension of the least squares residuals $r_i(x)$ is between 2 and 65.
% No problem is overrepresented in the sense that no function $k_p$ appears more than six times.
% Moreover, no pair ($k_p$, $n_p$) appears more than twice.

% $$ f(x) = \sum_{i=1}^{m} r_i(x)^2 $$

% where $r_i(x)$ are the residuals of the least squares problem.
% Only three of the 53 problems have local minimizers that are not global minimizers.
% These are based on two functions: (i) the Freudenstein and Roth function and (ii) the Brown almost-linear function.
% The remaining 50 problems have a unique global minimizer. in estimagic,
% linear rank one 4 problems based on linear_rank_one and linear_rank_one_zero_columns_rows

\noindent As mentioned above, we are interested in applications where the objective function is expensive and derivatives are not readily available. Hence, an appropriate convergence test should measure the decrease in the objective function rather than the gradient.
Thus, we adopt the following convergence test from \cite{MoreWild2009}:

% If True the runtime each algorithm needed for each problem is scaled by the time the fastest algorithm needed

\begin{align}
        f(x_0) - f(x) \geq (1 - \tau)(f(x_0) - f_L)
        \label{eq:benchmarking-convergence-test}
\end{align}
where $\tau$ is a tolerance level, $x_0$ is the vector of start parameters, and $f_L$ is the lowest objective function value obtained by any solver within a given runtime budget $\mu$, e.g. the number of function evaluations.

\noindent \ref{eq:benchmarking-convergence-test} measures the reduction in the objective function $f(x_0) - f(x)$ achieved by the solution $x^*$, i.e. the difference in $f$ between start and solution, relative to the reduction in $f$ compared to best possible reduction $f(x_0) - f_L$.
\footnote{Note that the convergence test in \ref{eq:benchmarking-convergence-test} is invariant to the affine transformation $f \rightarrow \alpha f + \beta$ with the scalars $\alpha > 0$ and $\beta$.}
MW argue that when the evaluation of $f$ is expensive it is not appropriate to set $f_L$
to an accurate estimate of $f$ at a local minimizer, e.g.obtained by a derivative-based solver.
Since no solver may be able to achieve this value within the user's computational budget.
Thus, MW recommend setting $f_L$ to the lowest objective function value obtained by any solver.
In practical situations, when
the evaluation of $f$ is expensive, a low-accuracy solution is all that can be obtained within
the user's computational budget. Even when $f$ is cheap, a derivative-free solver is not likely to achieve the precision a derivative-based solver may be able to achieve.
When $f_L$ is the smallest value of $f$ obtained by any solver, then at least one solver will satisfy \ref{eq:benchmarking-convergence-test} for any $\tau \geq 0$ (\cite{MoreWild2009}).\\
\noindent Moreover, when the objective evaluation is expensive and takes a long time it is sensible to scale the runtime of each algorithm by the time the fastest algorithm needed to solve the problem (\cite{MoreWild2009}). This way, the relative performance of the optimizers is highlighted. The scaled runtime of each solver is thus the ratio of the runtime of the solver and the runtime of the fastest algorithm for a given problem. The best solver for a particular problem naturally attains a scaled runtime of $1$, the lower bound.
%Any solver that fails to satisfy the convergence test on a given problem within the runtime budget is assigned an infinite runtime.

\noindent As mentioned above, users with costly optimization problems are not only interested in the relative performance of the optimizers but also in the performance of solvers as a function of the number of a given runtime measure. In other words, these users are eager to know the percentage of problems that can be solved both for a given tolerance $\tau$ and within a given computational budget, bounded by $\mu$. Profile plots are a compelling tool to visualize both the relative performances of a set of optimizers and their performance in terms of a limited runtime budget. By convention, any optimizer that fails to satisfy the convergence test on a given problem within $\mu$ is assigned an infinite runtime.
Profile plots provide an accurate view of the relative performance of solvers for a given $\tau$ and $\mu$. They show on the x-axis the scaled runtime of each algorithm and on the y-axis the percentage of problems each algorithm solved within the given runtime budget. For a runtime of $1$, the y-intercept indicates the fraction of problems out of all problems in the benchmark set the given optimizer solved the fastest compared to the other solvers in the competition.
E.g., an intercept of 0.5 indicates that the optimizer solved half of the problems the fastest within the user's runtime budget. For the maximum runtime of $\mu$, profile plots show the percentage of problems solved by each algorithm making use of the entire runtime budget.


% MW introduce profile plots as a tool for evaluating the performance of derivative-free optimization solvers when computational resources are limited.
% These plots answer the question: What percentage of problems can each algorithm
% solve within a certain runtime budget? The runtime budget is plotted on the $x$ axis and the share of problems solved by each solver on the $y$ axis.
% As outlined above, the runtime of each algorithm needed for each problem is scaled by the time the fastest algorithm needed to solve the problem.
% Note that when an optimizer fails to satisfy \ref{eq:benchmarking-convergence-test} within the given computational budget, it is scored as needing infinite runtime.
% converge according to a stopping criterion
% failing to converege according to the given stopping criterion and prediction is scored as needing an infinite computational budget.
% Convention = infinity when solver s fails to satisfy the convergence test on problem p
% (within a given limit? only for performance profile).

% \noindent Data profile plots are akin to cumulative distribution functions. Algorithms that perform well on some
% problems but are not able to solve others, will have steep increases in the beginning and then flat lines.
% On the other hand, algorithms that are robust but slow will have low shares in the beginning but may reach a percentage close to 100\% in the end. I.e. making use of the entire computational budget they can solve a large share of problems.



%     runtime_measure (str): "n_evaluations", "n_batches" or "walltime".
%         This is the runtime until the desired convergence was reached by an
%         algorithm. This is called performance measure by MorÃ© and Wild (2009).
%     normalize_runtime (bool): If True the runtime each algorithm needed for each
%         problem is scaled by the time the fastest algorithm needed. If True, the
%         resulting plot is what MorÃ© and Wild (2009) called data profiles.
%     stopping_criterion (str): one of "x_and_y", "x_or_y", "x", "y". Determines
%         how convergence is determined from the two precisions.
%     x_precision (float or None): how close an algorithm must have gotten to the
%         true parameter values (as percent of the Euclidean distance between start
%         and solution parameters) before the criterion for clipping and convergence
%         is fulfilled.
%     y_precision (float or None): how close an algorithm must have gotten to the
%         true criterion values (as percent of the distance between start
%         and solution criterion value) before the criterion for clipping and
%         convergence is fulfilled.




% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/competition_ls.pdf}
%     \caption{Benchmarking results for the more-wild benchmark set. The x-axis shows the number of function evaluations
%     normalized by the best result. The y-axis shows the share of problems solved.}
%     \label{fig:competition_ls}
% \end{figure}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/scalar_and_ls.pdf}
%     \caption{Benchmarking results for the more-wild benchmark set. The x-axis shows the number of function evaluations
%     normalized by the best result. The y-axis shows the share of problems solved.}
%     \label{fig:scalar_and_ls}
% \end{figure}
