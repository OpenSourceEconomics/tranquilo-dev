
\section{Motivation}
\label{sec:motivationcd}

\begin{itemize}
    \item the four problems from slides
    \item properties of residuals: continuously differentiable but Jacobian not available
    \item examples
        \begin{itemize}
            \item check for examples of pure least-squares problem application
            \item MSM and GMM and cholesky trick
            \item calibration of engineering problems (Janos)
            \item the covid model
        \end{itemize}
\end{itemize}


\begin{itemize}
    \item Least-squares optimization problems lie within the general class of \textit{blackbox} optimization problems:
        \begin{align}
            \min\limits_{l\leq x\leq u}f(x)
            \label{eq:problem-scalar-det}
        \end{align}
    where $f:\mathbb{R}^p\rightarrow\mathbb{R}$. In the least-squares case, a specific structure of the objective function $f$ is assumed:
        \begin{align}
            \min\limits_{l\leq x\leq u}f(x)=\min\limits_{l\leq x\leq u}\lVert r(x)\rVert^2=\min\limits_{l\leq x\leq u}\sum\limits_{i=1}^kr_i(x)^2
            \label{eq:problem-ls-det}
        \end{align}
    where the residual function $r(x)\equiv[r_1(x),\dots,r_k(x)]^T:\mathbb{R}^p\rightarrow\mathbb{R}^k$ is assumed to be a continuously differentiable function with unknown Jacobian matrix $J$.
    \item Stochastic least-squares problems assume that we observe only noisy evaluations of the true residuals and thus solve for the optimum of the expected value of the aggregate function:
        \begin{align}
            \min\limits_{l\leq x\leq u}\mathbb{E}f(x,\xi)=\min\limits_{l\leq x\leq u}\mathbb{E}\lVert r(x,\xi)\rVert^2=\min\limits_{l\leq x\leq u}\mathbb{E}\sum\limits_{i=1}^kr_i(x, \xi_i)^2
            \label{eq:problem-ls-noise}
        \end{align}
    where the noise vector $\xi\equiv[\xi_1,\dots,\xi_k]\sim\Xi$
\end{itemize}
