\subsection{Contributions}
\label{subsec:intro-contributions}
\paragraph{Algorithmic contributions} In the noise-free case, our algorithm improves upon existing model-based derivative-free optimization algorithms in the following ways:


\textit{History Search:} We implement variable trust-region sampling and thus save function evaluations by retaining the entire history of sampled points and searching for candidate points in the pool of existing sample points.



\textit{Parallelization:} Our algorithm allows for concurrent evaluations of the criterion functions. We make use of parallelization to obtain the initial sample and to implement two critical ideas during the acceptance step: 1) To do a line search for cases when the candidate point lies close to the trust region border, 2) Subject to available "idle" resources, to sample in the speculative trust region around the candidate point.


\textit{Adaptive Sampling:} For noisy optimization problems, our algorithm implements adaptive sampling of Monte Carlo simulation points to calculate the expected value of the stochastic objective function. In each iteration, the sample size is determined based on a simulated measure of improvement in the criterion value that comes from stochastic error only. In other words, we assume that the surrogate model perfectly captures the true criterion function on the current trust region without an approximation error.


\paragraph{Software Engineering} We offer several improvements over the existing optimization packages. The most important feature of our software architecture are:


\textit{Interface Clarity:} We implement each step of the optimization algorithm in blocks with clearly defined interfaces and easily replaceable components. Our software is thus especially accessible for domain experts without advanced knowledge of optimization algorithms.

\textit{Scalar Optimizer} Although we cover only least-square optimizers in this work, the highly flexible nature of our software architecture makes it possible to integrate a scalar optimization algorithm into the existing code base with only a few changes.

\paragraph{Open Source Contributions} We make our algorithm available on GitHub under the MIT License, which imposes only minimal software reuse restrictions. Tranquilo is available both as a standalone package and as part of estimagic, an open-source Python package for nonlinear optimization that provides a unified interface for a wide range of optimization algorithms, including the competitors of Tranquilo, against which we have run our benchmarks.

Besides the main algorithm, we provide an open access to Python implementations of BNTR and GQTPAR sub-solvers. In our implementation, we render these algorithms Numba compatible to use just in time compilation and overcome computational bottlenecks.


Finally, we make available the pure-Python implementations of the problem sets that we run our benchmark comparisons on. These include benchmark set from \citet{Mor2009}, as well as the medium-sized test problems from \citet{Cartis2019}.
%     \item Adaptive noise handling
%         \begin{itemize}
%             \item use the surrogate model and the variance estimate to simulate a noisy sample
%             \item fit a sub-model to the simluated sample and optimize it to get a candidate point
%             \item calculate the ration of improvement in the surogate model over the the improvement of sub-model as improvement for a problem with only noise but no approximation error
%             \item increase sample size if most simulated $\rho$-s are low
%         \end{itemize}

%     \item we retain entire history.
%     \item we save function evaluations with adaptive noise handling and avoid having idle cores with parallelization
%     \item Software engineering
