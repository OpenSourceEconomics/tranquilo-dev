\subsubsection{Fitting}
\label{subsubsec:fitting}
\hfill\\


In most of the existing model-based trust region algorithms, fitting the vector model is equivalent to solving some variant of an ordinary least squares problem:
\begin{align}
    \min\limits_{\theta}\sum\limits_{i=1}^{n_t}\lVert M_t^v(x_i-x_t^*,\theta)-r(x_i)\rVert^2
    \label{eq:ls-fitting-problem}
\end{align}
Where we use the convention $x_0=x_t^*$, and, with abuse of notation, $\{x_i\}_{i=0}^{n_t}$ represent the parameter vectors scaled to the topological space $B$ (a hypersphere or a hypercube), as discussed in section \ref{subsec:tr-framework}.

In case of linear residual models, the model parameters $\theta$ in \ref{eq:ls-fitting-problem} include an intercept and linear terms:
\begin{align}
    M_t^i(s)= c^i_t+g_t^is^T
    \label{eq:resid-model-linear}
\end{align}
In the case of quadratic residual models, $\theta$ additionally includes the second-order terms:
\begin{align}
    M_t^i(s)= c^i_t+g_t^is^T+\frac{1}{2}s^TH_t^is
    \label{eq:resid-model-quadratic}
\end{align}

To obtain a fully determined system of equations in \ref{eq:resid-model-quadratic}, one needs to evaluate the objective function at $(p+1)(p+2)/2$ interpolation points. With expensive objective functions, the entailed computational cost can be prohibitively expensive.

To overcome this computational cost, some algorithms have developed methods to build approximation models with fewer points than  $(p+1)(p+2)/2$, resolving the uncovered degrees of freedom by minimizing various criteria.

A popular approach employed in BOBYQA is to take up the unresolved degrees of freedom by minimizing the Frobenius norm of the change in the square terms of the model \citep{Powell2003}:
\begin{align}
    \min\lVert H-H_{t-1}\rVert_F = \min\sqrt{\sum\limits_{i=1}^p\sum\limits_{j=1}^{p}(h^{ij}(x)-h^{ij}_{t-1}(x))^2}
    \label{eq:min-frob-diff}
\end{align}
where $h_{ij}$ is the element in the $i$-th row and $j$th column of the square matrix $H$. DFBOLS \citep{Zhang2010} uses this approach in a least-squares optimization problem to obtain quadratic approximations of the residual functions with $2p+1$ points.

Another approach, employed by POUNDERS, is to penalize the quadratic terms themselves, again using the Frobenius norm:
\begin{align}
\min\lVert H(x)\rVert_F =\min\sqrt{\sum\limits_{i=1}^p\sum\limits_{j=1}^{p}(h^{ij}(x))^2}
\label{eq:min-frob}
\end{align}
Detailed in \cite{Wild2008}, this approach is motivated by theoretical results that guarantee an approximation quality of the quadratic model.

Other algorithms tackling underdetermined quadratic models are discussed in \cite{Larson2019}. Among these, \cite{Powell2012} minimizes a semi-norm combining the Frobenius norm of the second order and the square norm of the linear terms of the surrogate model.

In Tranquilo, we use ordinary least squares fitting for just- and overidentified problems ($|\mathcal{X}_t^{model}|\geq DoF(M_t^{i}))$. We also nest the two approaches given by equations \ref{eq:min-frob-diff} and \ref{eq:min-frob}. Specifically, we minimize the Frobenius norm of second-order terms of a vector model, which, if specified, can be residualized using the model from the prior iteration, resulting in \ref{eq:min-frob-diff}.

Motivated by the comparable success of the ordinary least squares and the penalization of second order terms\textcolor{red}{need concrete examples from literature or our experiments}, in Tranquilo we also implement a fitting method that combines the two approaches smoothly by a least squares problem that penalizes the linear terms less strongly. In practice, we apply a weighting matrix to the observations of the OLS problem that augments values in the first $p+1$ columns and shrinks the values in the last $p(p+1)/2-p-1$ columns.

In addition to the fitting methods discussed above, we make available two approaches particularly fit for noisy optimization problems: 1) Ridge regression and 2) Qauntile (median) regression.

Ridge regression performs a square norm regularization by solving the following problem:
\begin{align}
    \min\limits_{\theta}\sum\limits_{i=1}^{n}(M^v(x_i;\theta)-r(x_i))^2+\lambda\sum\limits_{i=1}^{m_p}\theta_i^2
    \label{eq:fit-ridge}
\end{align}
Where $\theta$ is the parameter vector, and the constant $\lambda$ is a penalty coefficient that controls the parsimony of the resulting model. The regularization offered by Ridge regression is relevant when we do not want to give equal weights to all the function evaluations since they may have high noise levels and not represent true function values.

Median regression or least absolute deviations estimator, defined as follows:
\begin{align}
    \min\limits_{\theta}\sum\limits_{i=1}^{n}|M^v(x_i;\theta)-r(x_i)|
    \label{eq:fit-lad}
\end{align}
offers robustness to outliers that can arise in noisy function evaluations.

Our main results discussed in further sections have been obtained with ordinary least squares and POUNDERS/BOBYQA fitting. To judge the competitiveness of our algorithm with the novel (in trust region literature) fitting methods will require fine-tuning of hyper-parameters which we leave for future work.
