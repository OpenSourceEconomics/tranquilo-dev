\subsubsection{Subsolver}
\label{subsubsec:subsolver}
\hfill\\

\noindent Recall that in each iteration $t$, we solve for the minimizer of the aggregate model $M_{t}^{s} (x_t + s)$ in $B(x_t, \Delta_t^{region})$, the subproblem, to obtain a new candidate step $s_t$ (see line 9 of Algorithm \ref{algo:core}).
% Before solving the surrogate problem, we center the trust region at the current iterate $x_t$, such that the trust region center is $0$ and the radius $\Delta^{region}_{t}$ is normalized to $1$.
Given how we fit the vector model in line 7 of Algorithm \ref{algo:core} (see \ref{subsubsec:fitting}), the optimization problem faced by the subsolver is either defined on (i) a sphere or (ii) a cube:


\begin{align}
    B := \begin{cases}
    \begin{aligned}
        & \{x \in \mathbb{R}^p \, : \, \lVert x \rVert_2 \leq 1 \} & , \ & \text{if sphere} \\
        & [-1, 1]^p & , \ & \text{if cube}
    \end{aligned}
    \end{cases}
    \label{eq:ball-metric}
\end{align}

\noindent where $\lVert\cdot\rVert_2$ is the Euclidean norm of a vector.
% Put differently, in (i) the trust region is a spherical ball defined via a radius constraint, and in (ii) the trust region is an $n$-dimensional hypercube defined on the interval $[-1, 1]^n$. In (ii), we additionally impose the box constraints $l \leq s \leq u$.
Note that the mapping of the parameter space onto the trust region ball is already done in the fitting step; see line 7 of Algorithm \ref{algo:core}.
The surrogate, or sub-, problem is defined by the quadratic model:

\begin{align}
    M_{t}^{s} (s) &= f_t + g_t^T s + \frac{1}{2} s^T H_t s \\
    &\text{s.t.} \quad \begin{aligned}[t]
        & s \in B
    \end{aligned}
    \label{eq:quad-subproblem}
\end{align}


\noindent where $g_t$ is the gradient of the aggregate model at the current iterate $x_t$ and $H_t$ is the Hessian of the aggregate model at $x_t$.
Note that \ref{eq:quad-subproblem} nests the linear model by setting $H_t=0$.
% The norm $\lVert\cdot\rVert$ denotes, depending on whether we are in the unconstrained or constrained case, the 2-norm or the $\infty$-norm, respectively. In the unconstrained case, since we normalize the trust region to have a center of 0 and a radius of 1, the box constraint simplifies to $-1 \leq s \leq 1$.
%In the constrained case, on the other hand, the bounds $ -1 < s < 1$  are active.
To deal with both cases adequately, we use two quadratic subsolvers that determine an almost exact solution to \ref{eq:quad-subproblem}: (i) GQTPAR in the sphere case and (ii) BNTR in the cube case.
Both solvers return the candidate step $s_t$ that minimizes the quadratic model \ref{eq:quad-subproblem}. In the subsequent discussion, we drop the subscript $t$ for brevity.
% In the subsequent discussion, we leave the dependence on the trust region center $x_t$ implicit and drop the subscript $t$ for brevity.

\noindent Among the host of modern trust region optimizers, POUNDERS (\cite{Wild2015}) is the only other algorithm that switches between the spherical and the cubic case. In fact, POUNDERS employs GQTPAR for (i) and BNTR for (ii), as we do. Other common trust region optimizers, however, only handle either (i), such as NEWUOA (\cite{newuoa2006}), or (ii), e.g. DFOLS (\cite{dfols2019}). NEWUOA utilizes the spherical subsolver TRSAPP and DFOLS employs the cubic TRSBOX. These subsolvers find approximate solutions to the subproblem in \ref{eq:quad-subproblem} via the conjugate gradient method.
Note that the solution time of the surrogate problem typically poses a computational bottleneck in derivative-free optimization when the objective function is expensive to evaluate.
We provide efficient, numba-accelerated reimplementations of both GQTPAR and BNTR, which yield almost exact solutions to \ref{eq:quad-subproblem}.



\paragraph{GQTPAR}

In the sphere case, GQTPAR determines an exact solution to the trust region
subproblem in \ref{eq:quad-subproblem}. We do, however, not iterate until a high-accuracy solution is found. We rather terminate after a few iterations when a satisfactory approximation to the true solution is obtained.\cite{More1983} show that the exact solution $s^*$ of the surrogate problem \ref{eq:quad-subproblem} satisfies

% A number of things are immediately apparent about (7.2.1). The solution we are
% seeking lies either interior to the trust region, that is ||s||2 < A, or on the boundary,
% ||s||2 = A. If the solution is interior, the trust-region bound may as well not have
% been there, and therefore SM is the unconstrained minimizer of q(s). But this can only
% happen if q(s) is convex, that is, if the Hessian H is positive semidefiniteâ€”we could be
% more precise here, but the details will shortly emerge. At once we see that a minimizer
% of a convex model problem may have a different character from that of a nonconvex
% one. In the nonconvex case, a solution must lie on the boundary of the trust region,
% while in the convex case a solution may or may not do so.


\begin{align}
    ( H + \lambda I ) \ s^* = - g
    \label{eq:exact-solution}
\end{align}

\noindent for some $\lambda \geq 0$, with the matrix $( H + \lambda I )$ being positive semidefinite and
$\lambda (\Delta^{region}_{t}  - \lVert s^* \rVert) = 0$. The latter is a complementary slackness condition
which states that at least one of the quantities $\lambda$ and $(\Delta^{region}  - \lVert s^* \rVert)$
must be zero at the optimum.
When the solution $s^*$ is interior to the trust region, i.e. $\lVert s^* \rVert < \Delta^{region}$,
then $\lambda^*= 0$ and the algorithm can be terminated immediately.
Otherwise, when $s^*$ lies on the boundary of the trust region, i.e. $\lVert s^* \rVert = \Delta^{region}$,
$\lambda > 0$ and Newton's method is applied to find the value of $\lambda$ such that
$\lVert s^* \rVert = \Delta^{region}$ is satisfied.
The unique solution is then defined by

\begin{align}
    s (\lambda) = -(H + \lambda I)^{-1} \ g
    \label{eq:boundary-solution}
\end{align}

\noindent for $\lambda> 0$ sufficiently large so that $H + \lambda I$ is positive definite and $ \lVert s (\lambda) \rVert = \Delta^{region}$.
More precisely, we find the optimal $\lambda^*$ using Newton's root finding method on the equation
\begin{align}
    \phi(\lambda)=\frac{1}{\Delta^{region}}-\frac{1}{\|s(\lambda)\|}
\end{align}

\noindent which is almost linear around $\lambda^*$.\footnote{Note that Newton's method is not applied to

\begin{align}
    \phi_0 (\lambda) = \lVert s (\lambda) \rVert - \Delta^{region}
    \label{eq:root-finding}
\end{align}

\noindent directly, because $\phi_0$ may be non-linear around $\lambda^*$. For details, see \cite{More1983} or \cite{Nocedal2006}.}
In order to apply Newton's method to \ref{eq:root-finding}, we need to evaluate both the function $\phi(\lambda)$ and its first derivative, $\phi'(\lambda)$. Both values are found by solving systems of linear equations.
First, we need to solve for $s(\lambda)$ in \ref{eq:boundary-solution} (see line 5 of Algorithm \ref{algo:gqtpar}), which we then plug into \ref{eq:root-finding} to calculate $\phi(\lambda)$.
In a second step, we solve for $q$ in line 10 of Algorithm \ref{algo:core}, which is required for deriving $\phi'(\lambda)$. The details are omitted here for brevity but are available in \cite{Conn2000}.


\begin{algorithm}
    \caption{GQTPAR algorithm - The "easy case"} \label{algo:gqtpar}
    \KwIn{Initial guess $s_0$, $\lambda^{(0)}$,  $\lambda^{(0)}_L$, $\lambda^{(0)}_U$}
    \For{$\ell$=0,1,2,...}{

        Safeguard $\lambda^{(l)}$ to obtain $\lambda^{(\ell)}_S$ \\
        \If{$H + \lambda^{(\ell)} I$ is positive definite}{
            Factor $H + \lambda^{(l)} I = R^T \ R$ \\
            % Solve $R^T \ R \ s_\ell = - g $
            Solve $s_\ell = - (R^T R)^{-1} g $ \\
            % \If{$\lVert s_\ell \rVert < \Delta^{region}$}{
                %     Compute $\tau$ and $\hat{z}$
                % }
                }
                Update $\lambda^{(\ell)}_L$, $\lambda^{(\ell)}_U$, $\lambda^{(\ell)}_S$ \\
                Check convergence criteria \\
                \uIf{$H + \lambda^{(l)} I$ is positive definite and $g \neq 0$ }{
                    Solve $q_\ell = (R^T)^{-1} s_\ell $ \\
                    Set $\lambda^{(\ell+1)}=\lambda^{(\ell)}+\left(\frac{\left\|s_{\ell}\right\|}{\left\|q_{\ell}\right\|}\right)^2\left(\frac{\left\|s_{\ell}\right\|-\Delta^{region}_{t}}{\Delta^{region}_{t}}\right)$
                    }
                    \uElse{
                        Set $\lambda^{(\ell + 1)} = \lambda^{(\ell)}_S$
                        }
                        }
                    \end{algorithm}

\noindent Before solving for $\lambda^*$, we need an expression for $s(\lambda$) in \label{eq:boundary-solution}. We obtain the candidate $s_\ell$ by factorizing the model Hessian via Cholesky factorization and solving the resulting linear system (see lines 5 and 6 of Algorithm \ref{algo:gqtpar}). Note that in line 4, we safeguard $\lambda^{(\ell)}$. This is necessary to ensure that $H + \lambda^{(\ell)} I$ is positive definite and the Cholesky factorization exists. For details on the safeguarding procedure, see \cite{More1983} or \cite{Nocedal2006}.


\noindent There may situations, however, where $H + \lambda^{(\ell)} I$ is positive definite, but no solution to $ \lVert s (\lambda)  \rVert = \Delta^{region} $ exists. This is what \cite{More1983} call the "hard case", which is not described in Algorithm \ref{algo:gqtpar}. We refer the interested reader to \cite{More1983} and \cite{Conn2000} for details on the hard case. In short, the solution is to find an eigenvector $z$ of $H$ corresponding to the eigenvalue $\lambda_1$. Then

\begin{align}
    \left(H-\lambda_1 I\right)(s+\tau z)=-g
    \label{eq:hard-case}
\end{align}

\noindent and $ \lVert s +\tau z \ (\lambda) \rVert  = \Delta^{region}  $ for some $\tau$.


% $\lambda^{*} = ( -\lambda_1, \infty \)$
% $ \lVert s^* \rVert = \delta$
% $ \lVert p \( \lambda \) \rVert  - \Delta = 0 $
% $  \frac{1}{\lVert p \( \lambda \) \rVert}  - \frac{1}{\Delta} = 0 $



\paragraph{Bounded Newton Trust Region (BNTR)}

% BNTR was developed for the Toolkit of Advanced Optimization (TAO) {tao-user-ref} and
% originally written in C. Tranquilo uses a fast, CPU-accelerated pure-python implementation.
BNTR is a trust region method for bound-constrained optimization of quadratic functions.\footnote{By enforcing box constraints on the parameter space, the spherical trust region constraint \\
$\lVert s \rVert \leq \Delta^{region}_{t}$ in the 2-norm changes to a cubic constraint in an $\infty$-norm sense.}
% These bound, or box, constraints are of the form
% $$\exists i \text{ such that } l_i \leq s_i \leq u_i$$
% $l_i \leq s_i \leq u_i$, where $l_i$ and $u_i$ are the lower and upper bounds on the $i$th parameter, respectively.
% Note that not all $s_i$ must, but may, be bounded. For any unbounded parameters, the corresponding inactive lower and upper bounds are set to $-1$ and $1$, the bounds of the cubic trust region, respectively.

% Internally, BNTR employs two types of projection methods to ensure that the search direction is feasible (cite taoref): (i) gradient projection
% $$
% \mathfrak{P}(g)= \begin{cases}0 & \text { if }\left(s \leq l_i \wedge g_i>0\right) \vee\left(s \geq u_i \wedge g_i<0\right) \\ g_i & \text { otherwise }\end{cases}
% $$
% and (ii) bound projection
% $$
% \mathfrak{B}(s)= \begin{cases}l_i & \text { if } s_i<l_i \\ u_i & \text { if } s_i>u_i \\ s_i & \text { otherwise }\end{cases}
% $$

% BNTR belongs to the class of bounded Newton-Krylov algorithms.
\noindent BNTR employs a trust-region-like approach combined with an active set method (\cite{TAO21}) to solve \ref{eq:quad-subproblem}. Instead of fitting a full surrogate model, BNTR uses a simplified quadratic model in the surrogate step. I.e. it solves for the minimizer $r$ of the reduced quadratic model $M^{r}_{t}(r)$ for the unconstrained, i.e. inactive, search directions only (see line 5 of Algorithm \ref{algo:bntr}) via a conjugate gradient method. \footnote{The available methods are Conjugate Gradient, Steihaug-Toint, and TRSBOX.}
The reduced model is constructed using the reduced model gradient $g^r$ and the reduced model hessian $H^r$ based on the inactive set $\mathcal{I}(x)$.
In the vain of other trust region optimizers, acceptance of the candidate step $r_t$ is determined based on the ratio of the predicted and the actual reduction of the reduced model; see lines 7 and 8 of Algorithm \ref{algo:bntr}. If the ratio is larger than a threshold, the trust region radius is increased. Else, the trust region radius is reduced.
The active and inactive sets are defined as follows (\cite{Bertsekas1982}):
\begin{align*}
\text{lower bounded: } \mathcal{L}(s) & = \{i: s_i \leq l_i \wedge g(s)_i > 0\}, \\
\text{upper bounded: } \mathcal{U}(s) & = \{i: s_i \geq u_i \wedge g(s)_i < 0\}, \\
\text{fixed: } \mathcal{F}(s) & = \{i: l_i = u_i\}, \\
\text{active-set: } \mathcal{A}(s) & = \{\mathcal{L}(s) \cup \mathcal{U}(s) \cup \mathcal{F}(s)\}, \\
\text{inactive-set: } \mathcal{I}(s) & = \{1,2,\ldots,n\} \setminus \mathcal{A}(s).
\end{align*}

\noindent Consider further the bound projection on $s$:

$$
\mathfrak{B}(s)= \begin{cases}l_i & \text { if } s_i<l_i \\ u_i & \text { if } s_i>u_i \\ s_i & \text { otherwise }\end{cases}
$$

\noindent With the reduced conjugate gradient step $r$ in hand, the active parameters in $\mathfrak{B}(s)$ are then set to their respective lower, upper, or fixed bounds, which yields the candidate step $s$ (line 6 of Algorithm \ref{algo:bntr}). The process is repeated until convergence criteria are met.

\begin{algorithm} %[H]
    \caption{BNTR algorithm}\label{algo:bntr}
    \KwIn{Initial guess $s_0$, $\Delta^{sub}_0 > 0$}

    Take a finite number of gradient descent steps and update $s_0$, $\Delta^{sub}_0$\\
    \For{$\ell$=0,1,2,...}{

        Create active set of bounds \\
        Construct reduced gradient and reduced model hessian \\
        Solve for the optimum of the reduced model $M^r_\ell$:
        $r_{\ell}\approx arg\,\min\limits_rM_{\ell}^r(r) \quad \text{s.t.} \ \lVert r \rVert \leq \Delta^{sub}_{\ell}$\\
        Set active dimensions of $r_\ell$ to their respective lower and upper bounds \\

        Calculate $\kappa_\ell = \frac{M_{\ell}^r(r_{\ell - 1}) - M_{\ell}^r(r_\ell)}{M_{\ell}^s(r_{\ell - 1}) - M_{\ell}^s(r_\ell)}$,
        the ratio of predicted over actual reduction

        \uIf{$\kappa_\ell$ is larger than a threshold}{
        Accept step $s_{\ell + 1} = s_\ell + r_\ell$ \\
        Expand trust region radius: $\Delta^{sub}_{\ell+1}=\alpha^{inc}\Delta^{sub}_t$
        }

        \uElse{
        Reject step $s_{\ell + 1} = s_\ell$ \\
        Shrink trust region radius $\Delta^{sub}_{\ell+1}=\alpha^{dec}\Delta^{sub}_t$
        }
        Check convergence criteria \\
        }
\end{algorithm}
