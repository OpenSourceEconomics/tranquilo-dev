\section{Parallelization}\label{sec:parallelization}

In this section we describe what parallelization features our algorithm offers and how they are
implemented. We also discuss the cost model that is used when comparing the performance of our
parallelized algorithm to serial versions. We refrain from an analysis of a parallelized objective
function and consider the case where the parallelization solely takes place at the optimization
algorithm level. Note further, that the Tranquilo algorithm only parallelizes the evaluation of
the objective functions, reflecting our primary assumption that the runtime of evaluating the
objective function dominates the runtime of auxiliary computations.

Parallelization occurs whenever we have to evaluate the objective function in the algorithm. This
happens in three places: (1) At the beginning of the optimization, where we parallelize the repeated
evaluation of the starting point. (2) During the sample evaluation, where we evaluate the objective
function in parallel at different sampling points. And, (3) during the acceptance step, where we
utilize unused resources during the acceptance point evaluation to accelerate the next periods
sampling or to improve upon the current candidate point. The parallel algorithm is given in Listing
\ref{algo:parallel}, with deviations from the serial algorithm highlighted in
blue\comment[id=TM]{Update this when we change the color}.

Another least-squares optimizer that parallelizes some evaluations is the DF-OLS algorithm of
\cite{dfols2019}, which parallelizes over the set of initial sample points, but not afterwards.

Tranquilo offers two options to govern the parallelization behavior: The first, number of cores
$n^{cores}$, determines the number of threads that are used for parallelization. The second, batch
size $n^{batch}$, determines the number of objective evaluations we perform simultaneously. In most
cases, $n^{cores} = n^{batch}$ is a good choice, but we allow for the possibility to set $n^{batch}$
to a larger value. Using larger values for the batch size than for the number of cores allows us to
use a single-core machine to test the algorithm's behavior as if it were deployed on a machine with
many cores. Note that the algorithms behavior is not affected by the number of cores, but only by
the batch size.

The following parallelization strategies are motivated by the assumption, that one
objective evaluation is as time-consuming as performing $n^{batch}$ many.

\subsection{Implementation}\label{subsec:parallelization::implementation}

For the implementation description we require the definition of the \emph{next multiple} of an
integer. Let $m \in \mathbb{N}$ be such an integer; we define the smallest multiple of $n^{batch}$
that is larger or equal to $m$ as the \emph{next multiple} of that number with basis
$n^{batch}$.\footnote{Formally, the next multiple of $m$ is given by $\lceil m / n^{batch}\rceil
\times n^{batch}$.} Since we only consider the case where $n^{batch}$ is the basis, we will suppress
its explicit mention hereafter. As an example, the next multiple of $n^{batch} + 1$, is $2 \times
n^{batch}$.

\subsubsection{At Start}

Parallelization at the start of the algorithm occurs only for noisy problems. In the noisy case, the
option $n^{start} \in \mathbb{N}$ describes the number of evaluations that are performed at the
starting value to average out noise. In the parallel case this number is replaced by its next
multiple. See Lines \ref{algo:parallel::start-start} - \ref{algo:parallel::start-end} of Listing
\ref{algo:parallel} for the implementation.

\subsubsection{Sample evaluation}

When parallelization is enabled, the sample evaluations are performed in parallel. Moreover, instead
of sampling the requested number of points, we sample the next multiple of it, and therefore
evaluate the objective function at more points without further costs. See Line
\ref{algo:parallel:sampling} of Listing \ref{algo:parallel} for the implementation. The DF-OLS
algorithm of \cite{dfols2019} uses this parallelization strategy for their initial sample. They do
not, however, employ this strategy for subsequent sampling.

\subsubsection{Acceptance Step}\comment[id=TM]{This will be updated once the acceptance step section
is written.}

\begin{itemize}
    \item if batch size is 1 (no parallelization): do classic acceptance based on rhos
    \item else, check if the candidate point is at the tr border
    \item if yes, get number of line search points (at most 3, at least batch size-1) and define the
        grid on the line
    \item calculate number of unallocated function evals based on the number of points on the line
        search and batch size ($batch\_size-1-n_evals_line_search$) \comment[id=MP]{WHY
        $batch\_size-1$ ?}
    \item if the previous number is non-zero, do speculative sampling around the candidate point
        using search radius
    \item add the line search and speculative points, if any, to the history
    \item check if objective is smaller at any of the new points (spec+line search)
    \item if so, update candidate fval and x
    \item insert algorithm listing at the end
\end{itemize}

\newgeometry{left=15mm,right=15mm, bottom=1cm, top=3cm}
\begin{algorithm}
    \caption{Parallel Tranquilo algorithm}\label{algo:parallel}
    \KwIn{
    Starting point $x^*_0\in\mathbb{R}^p$, initial trust region radius $\Delta^{region}_0$, and
    target sample size $n^{target}$, starting point evaluations $n^{start}$, batch size $n^{batch}$.

    \emph{Other constants:} Search factor $\gamma^{search}$, minimum step size $s^{min}$, relative
    improvement cut-offs $0<\rho^{1}<\rho^2$, trust region expansion and shrinking factors
    $0<\gamma^{dec}<1<\gamma^{inc}$, sample increment $n^{drop}$, and maximum number of iterations
    $t^{max}$

    A convergence objective.
    }
    \begingroup
    \color{UBonnBlue}
    \If{$n^{start}>1$}{\label{algo:parallel::start-start}
        $n^{start} \leftarrow$ next multiple of $n^{start}$
    }
    Initialize history with $\mathcal{H}_0=\{(x^*_0,r(x_0^*)): i = 1, \dots,
    n^{start}\}$\label{algo:parallel::start-end}\\
    \endgroup
    \For{t=0,1,2,...}{
        Given $x_t^*$ and $\Delta_t^{search}=\gamma^{search}\Delta^{region}_{t}$, scan history for
        points in the search region: $\mathcal{X}^{existing}_t = \{x\in\mathcal{H}|\Hquad\lVert
        x^*_t-x\rVert\leq\Delta^{search}_t\}$\\

        Filter existing points: $\mathcal{X}^{filtered}_t\equiv Filter(\mathcal{X}^{existing}_t) =
        \{Filter(x)|\Hquad x\in \mathcal{X}^{existing}_t\}$\\

        \If{$|\mathcal{X}_t^{filtered}|<n^{target}$}{
            \begingroup
            \color{UBonnBlue}
            Sample new points in the trust region: $\mathcal{X}^{new}_t\equiv
            Sample(\mathcal{X}_t^{filtered},B_t,n^{target}, n^{batch})$\label{algo:parallel:sampling}
            \endgroup
        }

        Build a vector model $M_t^v\equiv Fit(\mathcal{X}_t^{model};r,B_t)$ on
        $\mathcal{X}^{model}_t\equiv\mathcal{X}^{filtered}_t\cup\mathcal{X}^{new}_t$\label{algo-core-line:fit}\\

        Aggregate the vector model: ${M}_t^s = Aggregate(M_t^v)$\label{algo-core-aggregate}\\

        Solve the surrogate problem: $s_t\approx \argmin \{M_t^s(s): \Hquad\lVert
        s\rVert\leq\Delta_t^{region} \}$\label{algo-core-line:solve}\\

        \If{$|\mathcal{X}^{model}_t|>n^{target}$}{
            \While{$\lVert s_t\rVert\leq s^{min}$}{
                Improve sample geometry:
                $\mathcal{X}_t^{improved}=ImproveGeometry(\mathcal{X}_t^{model},
                n^{drop},\Delta_t^{region})$ and set
                $\mathcal{X}_t^{model}=\mathcal{X}_t^{improved}$\label{algo-core-listing-improve-geo}\\

                Build a vector model: $M_t^v\equiv Fit(\mathcal{X}_t^{model};r,B_t)$\\

                Aggregate the vector model: ${M}_t^s = Aggregate(M_t^v)$\\

                Solve the surrogate problem: $M_t^s$: $s_t \approx \argmin\{M_t^s(s):\Hquad\lVert
                s\rVert\leq\Delta_t^{region} \}$\\
                }
        }


        \While{$\lVert s_t\rVert\leq s^{min}$}{

            Improve sample geometry:
            $\mathcal{X}_t^{improved}=ImproveGeometry(\mathcal{X}_t^{model},
            n^{drop},\Delta_t^{region})$\\

            Sample new points: $\mathcal{X}_t^{new} =
            Sample(\mathcal{X}_t^{improved},B_t,n^{drop})$\\

            Build a vector model: $M_t^v\equiv Fit(\mathcal{X}_t^{model};r,B_t)$ on
            $\mathcal{X}_t^{model}=\mathcal{X}_t^{improved}\cup\mathcal{X}_t^{new}$\\

            Aggregate the vector model: ${M}_t^s = Aggregate(M_t^v)$\\

            Solve the surrogate problem: $s_t\approx \argmin\{M_t^s(s):\Hquad\lVert
            s\rVert\leq\Delta_t^{region}\}$\\
        }
        Calculate $\Delta f_t\equiv f(x_t^*) - f(x_t^*+s_t)$ and $\Delta M^{s}_t\equiv M_t^s(x_t^*)
        - M_t^s(x_t^*+s_t)$\\

        \nonl \textbf{Acceptance decision}:\\

        \begingroup
        \color{UBonnBlue}
        $(Decision_t,x_{t+1}^*,\rho_t;\Hquad\cdot\Hquad) =  Accept(s_t,x_t^{*},\Delta f_t,\Delta
        M_t^s, n^{batch};\Hquad\cdot\Hquad)$
        \endgroup

        \uIf{$\rho_t\geq\rho^{2}$ and  $s_t$ is large}{
        Expand trust region radius: $\Delta^{region}_{t+1}=\gamma^{inc}\Delta^{region}_t$

        }
        \uElseIf{$\rho_t>\rho^1$}{
        Leave the radius unchanged: $\Delta_{t+1}^{region}=\Delta_t^{region}$
        }
        \uElse{
        Shrink trust region radius $\Delta^{region}_{t+1}=\gamma^{dec}\Delta^{region}_t$
        }
    \If{$t>t^{max}$ or $converged$}{\textbf{break}}
    }

    \end{algorithm}
\restoregeometry

\subsection{Benchmarking}\label{subsec:parallelization::benchmarking}

The benchmark plots have to be adjusted for the parallel case, as the number of objective
evaluations does not provide a good measure of runtime anymore. Instead, we use the number of batch
evaluations as a proxy for runtime, in line with our parallel cost model. This reflects our
assumption that in the parallel case the evaluation of a batch takes as much time as the evaluation
of a single point.

Figure \ref{fig:parallelization::benchmark} shows the benchmark results of our parallel algorithm on
the \cite{MoreWild2009} benchmark set; see Section \ref{subsec:benchmark-core-algo} for details on
benchmarking. We compare our parallel least-squares version for $n^{batch} \in \{2, 4, 8\}$ to the
serial DF-OLS algorithm. We parallelize at the sampling level and use the \emph{classic line search}
for the acceptance step.\comment[id=TM]{Update this when we write the Acceptance Step section.} As
in Section \ref{subsec:benchmark-core-algo}, the y-axis denotes the share of solved problems, while
the x-axis denotes the multiple of minimal number of \emph{batches} needed to solve the problem.
This implies that the intercept can be interpreted as the share of problems that the respective
algorithm was able to solve the fastest under our parallel cost model.

We notice that for the
Tranquilo algorithm, the larger the batch size the faster its runtime measure. This is not
surprising, as we are using more objective evaluations when we increase the batch size. For the
DF-OLS algorithm, one batch is equal to one evaluation, as it does not parallelize. Unsurprisingly,
the DF-OLS algorithm is slower than the Tranquilo algorithm for all batch sizes, as it utilizes
fewer objective evaluations per batch. However, the DF-OLS algorithm is able to successfully
solve one problem more than the Tranquilo algorithm. This is due to an accuracy, as the problem
disappears when using a lower tolerance level for convergence. Solving this issue is a work in
progress.

\begin{figure}
    \includegraphics[width=\textwidth]{../bld/figures/profile_plots/parallelization_ls}
    \caption{Benchmark plots for the parallel cost model.}
    \label{fig:parallelization::benchmark}
\end{figure}\comment[id=TM]{The plot looks cheap; should be updated.}
